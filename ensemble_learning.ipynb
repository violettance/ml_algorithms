{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept behind it is simple. Multiple machine learning models are combined to obtain a more accurate model.\n",
    "\n",
    "Bagging, boosting and stacking are the three most popular ensemble learning techniques. Each of these techniques offers a unique approach to improving predictive accuracy.\n",
    "\n",
    "As we know from the bias-variance trade-off, an underfit model has high bias and low variance, whereas an overfit model has high variance and low bias. In either case, there is no balance between bias and variance. For there to be a balance, both the bias and variance need to be low. Ensemble learning tries to balance this bias-variance trade-off by reducing either the bias or the variance.\n",
    "\n",
    "Ensemble learning improves a modelâ€™s performance in mainly three ways:\n",
    "\n",
    "- By reducing the variance of weak learners\n",
    "- By reducing the bias of weak learners,\n",
    "- By improving the overall accuracy of strong learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "- Parallel vs. Sequential: Bagging and Boosting use multiple base models in parallel, whereas Stacking uses them in a sequential manner.\n",
    "- Variance vs. Bias Reduction: Bagging reduces variance by training models independently, whereas Boosting focuses on reducing bias by sequentially correcting mistakes.\n",
    "- Feature Utilization: Stacking utilizes predictions from multiple base models as additional features for the meta-model, combining different model strengths.\n",
    "- Objective: Bagging and Boosting are ensemble methods that focus on improving individual model performance, while Stacking combines multiple models to create a more powerful unified model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Variance with Bagging\n",
    "\n",
    "One popular ensemble method is Bagging, which stands for Bootstrap Aggregating. Bagging is particularly effective at reducing variance in predictions and improving the overall accuracy and robustness of the model.\n",
    "\n",
    "#### Bagging Steps\n",
    "\n",
    "- Bagging: Create multiple subsets of the training data and train diverse base models.\n",
    "- Model Aggregation: Combine predictions through averaging (regression) or majority vote (classification).\n",
    "- Reducing Variance: Errors among diverse models cancel out, leading to more stable predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are using Bagging with Decision Trees as the base model to classify the \"load_digits\" dataset, which contains images of handwritten digits (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "# Load the \"load_digits\" dataset\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier as the base model\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create a Bagging classifier with 20 base estimators (trees)\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=20, random_state=42)\n",
    "\n",
    "# Train the Bagging model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Bagging Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Bias by Boosting\n",
    "\n",
    "Boosting is another powerful ensemble learning method that aims to reduce bias and improve the overall performance of machine learning models. Unlike Bagging, which focuses on reducing variance, Boosting focuses on reducing bias and increasing the model's accuracy.\n",
    "\n",
    "#### Boosting Steps\n",
    "\n",
    "- Iterative Learning: Boosting trains weak learners iteratively.\n",
    "- Weighted Training: Misclassified instances get higher weights.\n",
    "- Model Weighting: Better models receive higher weights.\n",
    "- Model Aggregation: Combine predictions using weighted averaging or voting.\n",
    "- Bias Reduction: Emphasize challenging instances to reduce bias.\n",
    "- Final Prediction: Strong model with improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are using AdaBoost with Decision Stumps (weak learners) as the base model to classify the \"Wine\" dataset, which contains features extracted from three types of wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Load the Wine dataset\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier as the base model\n",
    "base_model = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Create an AdaBoost classifier with 100 base estimators (weak learners)\n",
    "adaboost_model = AdaBoostClassifier(base_model, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the AdaBoost model\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"AdaBoost Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Model Accuracy with Stacking\n",
    "\n",
    "Stacking is an ensemble learning technique that combines multiple individual models to achieve better predictive performance.\n",
    "\n",
    "#### Stacking Steps\n",
    "\n",
    "- Create Diverse Base Models: Build multiple independent base models.\n",
    "- Generate Predictions: Train base models and get predictions for the target variable.\n",
    "- Create Meta-Model: Develop a meta-model using base models' predictions.\n",
    "- Blend Predictions: Train the meta-model to blend base models' predictions.\n",
    "- Make Final Predictions: Use the meta-model to predict new data by combining base models' predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are using a StackingClassifier to classify the \"Diabetes\" dataset, which contains features related to diabetes patients' medical details. We will convert it into a binary classification problem by predicting whether the disease progression will be greater than 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Accuracy: 0.7528089887640449\n"
     ]
    }
   ],
   "source": [
    "# Load the Diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Since it's a regression dataset, let's convert it into a binary classification problem\n",
    "y_binary = (y > 150).astype(int)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Define the meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Create the stacking classifier\n",
    "stacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model)\n",
    "\n",
    "# Train the stacking model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Stacking Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
