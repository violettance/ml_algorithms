{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learning refers to the situation in a classification problem where the distribution of classes in the training data is not balanced, meaning one class has significantly more samples than the other(s). In such cases, the model can become biased towards the majority class, leading to poorer performance in correctly predicting the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced learning can cause several issues:\n",
    "\n",
    "- Biased Predictions: Models tend to predict the majority class more frequently, leading to imbalanced and biased predictions.\n",
    "\n",
    "- Low Recall for Minority Class: The model might have a high accuracy due to correctly classifying the majority class but might fail to identify most instances of the minority class, resulting in low recall for the minority class.\n",
    "\n",
    "- Model Evaluation Misleading: Accuracy alone can be misleading in imbalanced datasets, as it can be high even if the model only predicts the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In imbalanced learning, several techniques can be used to address the class imbalance issue. These techniques can be broadly categorized into two main types:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Level Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under-Sampling:\n",
    "\n",
    "- Definition: Removing samples from the majority class to balance the class distribution. This can be done randomly or with more sophisticated methods like Cluster Centroids and Tomek links.\n",
    "\n",
    "- Purpose: Helps reduce the dominance of the majority class and can prevent the model from being biased towards it.\n",
    "\n",
    "- Pros: Simple and computationally efficient, can lead to faster training times.\n",
    "\n",
    "- Cons: May discard valuable information, potential loss of important instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.under_sampling import ClusterCentroids\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic imbalanced dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.99, 0.01], random_state=42)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster Centroids can be an effective under-sampling technique, especially when the majority class has clusters of data points close to the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iremkurt/anaconda3/envs/slit/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after under-sampling: 0.705\n"
     ]
    }
   ],
   "source": [
    "# Create the Cluster Centroids under-sampling object\n",
    "cluster_centroids = ClusterCentroids(random_state=42)\n",
    "\n",
    "# Apply under-sampling on the training set\n",
    "X_train_resampled, y_train_resampled = cluster_centroids.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution before and after under-sampling\n",
    "#Â print(\"Class distribution before under-sampling:\", pd.Series(y_train).value_counts())\n",
    "# print(\"Class distribution after under-sampling:\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after under-sampling:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomek Links are effective in dealing with class imbalance when there are overlapping instances of the majority and minority classes, particularly at their borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after Tomek Links under-sampling: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Create the Tomek Links under-sampling object\n",
    "tomek_links = TomekLinks()\n",
    "\n",
    "# Apply under-sampling on the training set\n",
    "X_train_resampled, y_train_resampled = tomek_links.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after Tomek Links under-sampling:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over-Sampling:\n",
    "\n",
    "- Definition: Creating additional samples for the minority class to balance the class distribution.\n",
    "\n",
    "- Purpose: Increase the representation of the minority class and improve the model's performance.\n",
    "\n",
    "- Pros: Helps the model learn from the minority class, can be combined with techniques like SMOTE and ADASYN.\n",
    "\n",
    "- Cons: May lead to overfitting, synthetic samples might not fully represent the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after over-sampling: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Create the SMOTE over-sampling object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Apply over-sampling on the training set\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution before and after over-sampling\n",
    "# print(\"Class distribution before over-sampling:\", pd.Series(y_train).value_counts())\n",
    "# print(\"Class distribution after over-sampling:\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after over-sampling:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combined Over-Under Sampling:\n",
    "\n",
    "- Definition: Combining over-sampling and under-sampling to balance the class distribution.\n",
    "\n",
    "- Purpose: Take advantage of both methods to improve performance.\n",
    "\n",
    "- Pros: Overcomes limitations of individual techniques, can lead to better results.\n",
    "\n",
    "- Cons: Requires careful tuning and experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTEENN is effective when the dataset suffers from both class imbalance and contains noisy samples that can potentially interfere with the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after combined resampling: 0.97\n"
     ]
    }
   ],
   "source": [
    "# Create the SMOTEENN object, which combines SMOTE and Edited Nearest Neighbors\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "# Apply combined under-sampling and over-sampling on the training set\n",
    "X_train_resampled, y_train_resampled = smote_enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution before and after combined resampling\n",
    "# print(\"Class distribution before combined resampling:\", pd.Series(y_train).value_counts())\n",
    "# print(\"Class distribution after combined resampling:\", pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after combined resampling:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic Data Generation:\n",
    "\n",
    "- Definition: Creating artificial data to address limited data availability and improve model performance.\n",
    "\n",
    "- Purpose: Mitigate class imbalance by providing more data for the minority class.\n",
    "\n",
    "- Pros: Potential for better generalization and improved model performance.\n",
    "\n",
    "- Cons: Risk of overfitting if not handled carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE works well with moderate class imbalance and linear decision boundaries, while ADASYN is suitable for severe class imbalance and complex decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after SMOTE over-sampling: 0.97\n",
      "Accuracy after ADASYN over-sampling: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Create the SMOTE over-sampling object\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Create the ADASYN over-sampling object\n",
    "adasyn = ADASYN(random_state=42)\n",
    "\n",
    "# Apply SMOTE to generate synthetic data for the minority class\n",
    "X_train_resampled_smote, y_train_resampled_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Apply ADASYN to generate synthetic data for the minority class\n",
    "X_train_resampled_adasyn, y_train_resampled_adasyn = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution before and after over-sampling with SMOTE\n",
    "# print(\"Class distribution before SMOTE over-sampling:\", pd.Series(y_train).value_counts())\n",
    "# print(\"Class distribution after SMOTE over-sampling:\", pd.Series(y_train_resampled_smote).value_counts())\n",
    "\n",
    "# Check class distribution before and after over-sampling with ADASYN\n",
    "# print(\"Class distribution before ADASYN over-sampling:\", pd.Series(y_train).value_counts())\n",
    "# print(\"Class distribution after ADASYN over-sampling:\", pd.Series(y_train_resampled_adasyn).value_counts())\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data using SMOTE\n",
    "rf_model_smote = RandomForestClassifier(random_state=42)\n",
    "rf_model_smote.fit(X_train_resampled_smote, y_train_resampled_smote)\n",
    "\n",
    "# Create a RandomForestClassifier and train on the resampled data using ADASYN\n",
    "rf_model_adasyn = RandomForestClassifier(random_state=42)\n",
    "rf_model_adasyn.fit(X_train_resampled_adasyn, y_train_resampled_adasyn)\n",
    "\n",
    "# Make predictions on the test set using SMOTE model\n",
    "y_pred_smote = rf_model_smote.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set using ADASYN model\n",
    "y_pred_adasyn = rf_model_adasyn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy for SMOTE model\n",
    "accuracy_smote = accuracy_score(y_test, y_pred_smote)\n",
    "print(\"Accuracy after SMOTE over-sampling:\", accuracy_smote)\n",
    "\n",
    "# Calculate accuracy for ADASYN model\n",
    "accuracy_adasyn = accuracy_score(y_test, y_pred_adasyn)\n",
    "print(\"Accuracy after ADASYN over-sampling:\", accuracy_adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Level Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Methods:\n",
    "\n",
    "- Definition: Combine models to address class imbalance and improve performance.\n",
    "\n",
    "- Purpose: Boost accuracy for the minority class using collective model knowledge.\n",
    "\n",
    "- Pros: Effective in handling class imbalance and improving overall performance.\n",
    "\n",
    "- Cons: May increase computational complexity and resource requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Weighting:\n",
    "\n",
    "- Definition: Assigning higher weights to the minority class and lower weights to the majority class during training.\n",
    "\n",
    "- Purpose: Adjust impact of different classes on the model's loss function.\n",
    "\n",
    "- Pros: Simple to implement, no changes to the dataset.\n",
    "\n",
    "- Cons: Not as effective in severe class imbalance, performance improvement may be limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with class weighting: 0.965\n"
     ]
    }
   ],
   "source": [
    "# Calculate class weights\n",
    "class_weight = dict(zip(np.unique(y_train), np.bincount(y_train)))\n",
    "\n",
    "# Create a DecisionTreeClassifier with class weights\n",
    "dt_model = DecisionTreeClassifier(class_weight=class_weight, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with class weighting:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost-Sensitive Learning:\n",
    "\n",
    "- Definition: Considering varying misclassification costs during training.\n",
    "\n",
    "- Purpose: Optimize model performance in scenarios with imbalanced misclassification costs.\n",
    "\n",
    "- Pros: Better decision-making in applications with imbalanced costs.\n",
    "\n",
    "- Cons: Requires accurate estimation of misclassification costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with cost-sensitive learning: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Define misclassification costs for each class\n",
    "class_costs = {0: 1.0, 1: 10.0}\n",
    "\n",
    "# Create a DecisionTreeClassifier with class weights based on misclassification costs\n",
    "dt_model = DecisionTreeClassifier(class_weight=class_costs, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with cost-sensitive learning:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly Detection:\n",
    "\n",
    "- Definition: Identify rare or unusual data points in imbalanced datasets. This can be done randomly or with more sophisticated methods like OneClassSVM and IsolationForest.\n",
    "\n",
    "- Purpose: Detect anomalies or outliers despite class imbalance.\n",
    "\n",
    "- Pros: Useful for identifying rare events or anomalies with low representation.\n",
    "\n",
    "- Cons: May struggle to distinguish genuine anomalies from novel patterns, especially in unsupervised anomaly detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Class SVM is particularly useful when dealing with unsupervised learning scenarios, where there are no labeled anomalies for training, it can can struggle with high-dimensional spaces due to the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with anomaly detection: 0.455\n"
     ]
    }
   ],
   "source": [
    "# Treat the minority class as an anomaly (class 1 is the minority class)\n",
    "anomaly_mask = y_train == 1\n",
    "\n",
    "# Create a One-Class SVM classifier\n",
    "svm_model = OneClassSVM()\n",
    "\n",
    "# Train the model on the majority class (normal instances)\n",
    "svm_model.fit(X_train[~anomaly_mask])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary (1 for anomalies, -1 for normal instances)\n",
    "y_pred[y_pred == 1] = 0  # Normal instances\n",
    "y_pred[y_pred == -1] = 1  # Anomalies\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with anomaly detection:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolation Forest is generally faster and more scalable, especially for high-dimensional data, provides more interpretability, requires minimal hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with Isolation Forest: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Treat the minority class as an anomaly (class 1 is the minority class)\n",
    "anomaly_mask = y_train == 1\n",
    "\n",
    "# Create an Isolation Forest classifier\n",
    "if_model = IsolationForest()\n",
    "\n",
    "# Train the model on the majority class (normal instances)\n",
    "if_model.fit(X_train[~anomaly_mask])\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = if_model.predict(X_test)\n",
    "\n",
    "# Convert predictions to binary (1 for anomalies, -1 for normal instances)\n",
    "y_pred[y_pred == 1] = 0  # Normal instances\n",
    "y_pred[y_pred == -1] = 1  # Anomalies\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy with Isolation Forest:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
